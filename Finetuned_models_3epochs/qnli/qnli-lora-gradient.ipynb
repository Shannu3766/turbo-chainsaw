{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install transformers\n!pip install peft\n!pip install evaluate\n!pip install -U datasets==2.20.0 pyarrow==15.0.2 transformers==4.44.2 evaluate==0.4.2 --no-cache-dir\n!pip install -q datasets evaluate accelerate scikit-learn pandas matplotlib\n!pip install -U \"transformers>=4.41\" accelerate safetensors\n!pip install -U bitsandbytes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install --upgrade --no-cache-dir git+https://github.com/Shannu3766/bi_influence.git\n!pip install --upgrade --no-cache-dir git+https://github.com/Shannu3766/Cosine-similarity.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom typing import Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport evaluate\nfrom adaptive_lora_gradient.callbacks import AdaptiveLoRACallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:23.600622Z","iopub.execute_input":"2025-12-05T15:54:23.600880Z","iopub.status.idle":"2025-12-05T15:54:23.605160Z","shell.execute_reply.started":"2025-12-05T15:54:23.600854Z","shell.execute_reply":"2025-12-05T15:54:23.604301Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model_checkpoint = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \noutput_dir = \"./tinyllama-qnli-lora\"\nseed = 42\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:23.605730Z","iopub.execute_input":"2025-12-05T15:54:23.605921Z","iopub.status.idle":"2025-12-05T15:54:23.620401Z","shell.execute_reply.started":"2025-12-05T15:54:23.605906Z","shell.execute_reply":"2025-12-05T15:54:23.619769Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"batch_size = 16\neval_batch_size = 32\nnum_train_epochs = 3\nmax_length = 128\nlearning_rate = 3e-5\nweight_decay = 0.01\nrank=8\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=rank,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=[   \n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:23.621114Z","iopub.execute_input":"2025-12-05T15:54:23.621440Z","iopub.status.idle":"2025-12-05T15:54:23.636591Z","shell.execute_reply.started":"2025-12-05T15:54:23.621422Z","shell.execute_reply":"2025-12-05T15:54:23.635987Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\ndef clean_text(s: Optional[str]) -> str:\n    if s is None:\n        return \"\"\n    return \" \".join(str(s).strip().split())\n\ndef build_one_shot_demo(example: dict) -> str:\n    q = clean_text(example[\"question\"])\n    c = clean_text(example[\"context\"])\n    lbl = example.get(\"label_text\", \"Yes\")\n    return f\"Example:\\nQuestion: {q}\\nContext: {c}\\nAnswer (Yes/No): {lbl}\\n\\n\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:23.637262Z","iopub.execute_input":"2025-12-05T15:54:23.637507Z","iopub.status.idle":"2025-12-05T15:54:23.656380Z","shell.execute_reply.started":"2025-12-05T15:54:23.637483Z","shell.execute_reply":"2025-12-05T15:54:23.655660Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"ONE_SHOT_EXAMPLE = {\n    \"question\": \"Who wrote Hamlet?\",\n    \"context\": \"Hamlet was written by William Shakespeare and first performed in the early 17th century.\",\n    \"label_text\": \"Yes\",  # \"Yes\" => the context contains the answer\n}\n\nfrom datasets import load_dataset\nimport time\n\ndef load_dataset_with_retry(path, name=None, max_retries=None, wait=2):\n    \"\"\"\n    Repeatedly tries to load a dataset until it succeeds.\n    - max_retries: None â†’ infinite retries\n    - wait: base wait time (exponential backoff)\n    \"\"\"\n    attempt = 0\n\n    while True:\n        try:\n            if name is not None:\n                ds = load_dataset(path, name)\n            else:\n                ds = load_dataset(path)\n\n            print(f\"Dataset loaded successfully after {attempt} attempts.\")\n            return ds\n\n        except Exception as e:\n            attempt += 1\n            wait_time = wait * min(5, attempt)  # exponential cap\n\n            print(f\"[Attempt {attempt}] Failed to load dataset: {e}\")\n            print(f\"Retrying in {wait_time} seconds...\\n\")\n\n            # If user gives a max_retries limit\n            if max_retries is not None and attempt >= max_retries:\n                print(\"Max retries reached. Raising error.\")\n                raise e\n\n            time.sleep(wait_time)\ndataset = load_dataset_with_retry(\"glue\", \"qnli\")\n# dataset = load_dataset(\"glue\", \"qnli\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:23.657317Z","iopub.execute_input":"2025-12-05T15:54:23.657644Z","iopub.status.idle":"2025-12-05T15:54:26.079147Z","shell.execute_reply.started":"2025-12-05T15:54:23.657616Z","shell.execute_reply":"2025-12-05T15:54:26.078450Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded successfully after 0 attempts.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n\nif tokenizer.pad_token is None:\n    # safe default: use eos_token as pad\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:26.081570Z","iopub.execute_input":"2025-12-05T15:54:26.081825Z","iopub.status.idle":"2025-12-05T15:54:26.380324Z","shell.execute_reply.started":"2025-12-05T15:54:26.081808Z","shell.execute_reply":"2025-12-05T15:54:26.379630Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\n# Load tokenizer\n\n\ndef preprocess_function(examples, *, max_length: int = max_length, add_demo: bool = False):\n    \"\"\"\n    Build instruction-like prompts and tokenize them.\n    - add_demo: if True, prepends a 1-shot example to each prompt (can help decoder-only models)\n    \"\"\"\n    questions = [clean_text(q) for q in examples[\"question\"]]\n    contexts = [clean_text(s) for s in examples[\"sentence\"]]\n\n    demo_str = build_one_shot_demo(ONE_SHOT_EXAMPLE) if add_demo else \"\"\n\n    prompts = []\n    for q, c in zip(questions, contexts):\n        # Instruction-style prompt ending with a short, constrained label target\n        prompt = (\n            \"You are a helpful assistant.\\n\"\n            f\"{demo_str}\"\n            f\"Question: {q}\\n\"\n            f\"Context: {c}\\n\"\n            \"Answer (Yes/No):\"\n        )\n        prompts.append(prompt)\n\n    tokenized = tokenizer(\n        prompts,\n        truncation=True,\n        max_length=max_length,\n        padding=False, \n    )\n\n    if \"label\" in examples:\n        tokenized[\"labels\"] = examples[\"label\"]\n    elif \"labels\" in examples:\n        tokenized[\"labels\"] = examples[\"labels\"]\n\n    return tokenized\n\nprint(\"Tokenizing dataset...\")\ntokenized = dataset.map(\n    lambda ex: preprocess_function(ex, max_length=max_length, add_demo=False),\n    batched=True,\n    remove_columns=[\"question\", \"sentence\", \"idx\"],\n)\n\nif \"label\" in tokenized[\"train\"].column_names and \"labels\" not in tokenized[\"train\"].column_names:\n    tokenized = tokenized.rename_column(\"label\", \"labels\")\n\ntokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", pad_to_multiple_of=8)\ntrain_dataset = tokenized[\"train\"].select(range(10000))\neval_dataset = tokenized[\"validation\"].select(range(1500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:26.381074Z","iopub.execute_input":"2025-12-05T15:54:26.381419Z","iopub.status.idle":"2025-12-05T15:54:27.021537Z","shell.execute_reply.started":"2025-12-05T15:54:26.381394Z","shell.execute_reply":"2025-12-05T15:54:27.020938Z"}},"outputs":[{"name":"stdout","text":"Tokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5463 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68115046dd3f4c54b44e7695f4104c3b"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"print(\"Loading model...\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_checkpoint,\n    num_labels=2,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16, \n    trust_remote_code=True\n)\n\nprint(\"Model Loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:27.666500Z","iopub.execute_input":"2025-12-05T15:54:27.667324Z","iopub.status.idle":"2025-12-05T15:54:28.768123Z","shell.execute_reply.started":"2025-12-05T15:54:27.667297Z","shell.execute_reply":"2025-12-05T15:54:28.767497Z"}},"outputs":[{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model Loaded\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"Model Loaded\")\n# Resize token embeddings if tokenizer changed\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.use_cache = False\n\n# -------------------------\n# Apply LoRA (PEFT)\n# -------------------------\nprint(\"Applying LoRA (PEFT)...\")\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:28.769257Z","iopub.execute_input":"2025-12-05T15:54:28.769849Z","iopub.status.idle":"2025-12-05T15:54:31.800164Z","shell.execute_reply.started":"2025-12-05T15:54:28.769830Z","shell.execute_reply":"2025-12-05T15:54:31.799384Z"}},"outputs":[{"name":"stdout","text":"Model Loaded\nApplying LoRA (PEFT)...\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"val_dataloader = torch.utils.data.DataLoader(\n    eval_dataset,\n    batch_size=eval_batch_size,\n    shuffle=False,\n    collate_fn=data_collator,\n    pin_memory=torch.cuda.is_available(),\n    num_workers=2,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:31.801570Z","iopub.execute_input":"2025-12-05T15:54:31.801931Z","iopub.status.idle":"2025-12-05T15:54:31.806302Z","shell.execute_reply.started":"2025-12-05T15:54:31.801906Z","shell.execute_reply":"2025-12-05T15:54:31.805497Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from peft.tuners.lora import LoraLayer\n\ndef get_lora_module_names(peft_model):\n    names = []\n    for name, module in peft_model.named_modules():\n        if isinstance(module, LoraLayer):\n            names.append(name)\n    return names\n\nlora_names = get_lora_module_names(model)   # `model` is your PEFT-wrapped model\nprint(f\"ðŸ”¢ Number of LoRA modules: {len(lora_names)}\\n\")\n\n# for n in lora_names:\n    # print(\" \", n)\nnum_lora_modules = len(lora_names)\nTOTAL_RANK_BUDGET = rank * num_lora_modules\n\navg_rank_per_module = TOTAL_RANK_BUDGET / num_lora_modules\nprint(\n    f\"ðŸ’¡ With TOTAL_RANK_BUDGET={TOTAL_RANK_BUDGET} over \"\n    f\"{num_lora_modules} modules, avg rank â‰ˆ {avg_rank_per_module:.2f}\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:31.808144Z","iopub.execute_input":"2025-12-05T15:54:31.808376Z","iopub.status.idle":"2025-12-05T15:54:31.830789Z","shell.execute_reply.started":"2025-12-05T15:54:31.808359Z","shell.execute_reply":"2025-12-05T15:54:31.829904Z"}},"outputs":[{"name":"stdout","text":"ðŸ”¢ Number of LoRA modules: 154\n\nðŸ’¡ With TOTAL_RANK_BUDGET=1232 over 154 modules, avg rank â‰ˆ 8.00\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"adaptive_callback = AdaptiveLoRACallback(\n    val_dataloader=val_dataloader,\n    total_rank=TOTAL_RANK_BUDGET,\n    tau=0.9,\n    min_rank=4,\n    validate_batch_size=eval_batch_size,\n    verbose=True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:34.092672Z","iopub.execute_input":"2025-12-05T15:54:34.092964Z","iopub.status.idle":"2025-12-05T15:54:34.096993Z","shell.execute_reply.started":"2025-12-05T15:54:34.092940Z","shell.execute_reply":"2025-12-05T15:54:34.096189Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"accuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    acc = accuracy.compute(predictions=preds, references=labels)\n    return {\"accuracy\": acc[\"accuracy\"]}\n\n# -------------------------\n# TrainingArguments + Trainer\n# -------------------------\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=eval_batch_size,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    num_train_epochs=num_train_epochs,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    save_total_limit=2,\n    metric_for_best_model=\"accuracy\",\n    seed=seed,\n    fp16=False,\n    bf16=True,\n    push_to_hub=False,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[adaptive_callback]\n)\n\nprint(\"Starting training...\")\ntrainer.train()\n\nprint(\"Saving model and adapters...\")\ntrainer.save_model(output_dir)\nprint(\"Saved model to\", output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:54:34.308685Z","iopub.execute_input":"2025-12-05T15:54:34.309367Z","iopub.status.idle":"2025-12-05T18:24:49.429141Z","shell.execute_reply.started":"2025-12-05T15:54:34.309337Z","shell.execute_reply":"2025-12-05T18:24:49.428380Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_337/3391662561.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n\n--- AdaptiveLoRA: Preparing ranks for Epoch 1 ---\nComputing BI importance scores (pre-training)...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing Importance:   0%|          | 0/47 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Allocating new ranks based on BI scores...\nApplying new ranks to LoRA modules for this epoch...\n  - base_model.model.model.layers.0.self_attn.q_proj: r=8 â†’ 6 (Score: 0.2348)\n  - base_model.model.model.layers.0.self_attn.k_proj: r=8 â†’ 9 (Score: 0.4964)\n  - base_model.model.model.layers.0.self_attn.v_proj: r=8 â†’ 7 (Score: 0.3562)\n  - base_model.model.model.layers.0.self_attn.o_proj: r=8 â†’ 11 (Score: 0.7024)\n  - base_model.model.model.layers.0.mlp.gate_proj: r=8 (Unchanged, Score: 0.4105)\n  - base_model.model.model.layers.0.mlp.up_proj: r=8 (Unchanged, Score: 0.4128)\n  - base_model.model.model.layers.0.mlp.down_proj: r=8 (Unchanged, Score: 0.4128)\n  - base_model.model.model.layers.1.self_attn.q_proj: r=8 â†’ 7 (Score: 0.3473)\n  - base_model.model.model.layers.1.self_attn.k_proj: r=8 â†’ 15 (Score: 1.0000)\n  - base_model.model.model.layers.1.self_attn.v_proj: r=8 â†’ 9 (Score: 0.4820)\n  - base_model.model.model.layers.1.self_attn.o_proj: r=8 (Unchanged, Score: 0.3940)\n  - base_model.model.model.layers.1.mlp.gate_proj: r=8 â†’ 9 (Score: 0.4842)\n  - base_model.model.model.layers.1.mlp.up_proj: r=8 (Unchanged, Score: 0.4722)\n  - base_model.model.model.layers.1.mlp.down_proj: r=8 (Unchanged, Score: 0.4722)\n  - base_model.model.model.layers.2.self_attn.q_proj: r=8 (Unchanged, Score: 0.3864)\n  - base_model.model.model.layers.2.self_attn.k_proj: r=8 â†’ 14 (Score: 0.9314)\n  - base_model.model.model.layers.2.self_attn.v_proj: r=8 â†’ 9 (Score: 0.4915)\n  - base_model.model.model.layers.2.self_attn.o_proj: r=8 (Unchanged, Score: 0.4374)\n  - base_model.model.model.layers.2.mlp.gate_proj: r=8 (Unchanged, Score: 0.4372)\n  - base_model.model.model.layers.2.mlp.up_proj: r=8 (Unchanged, Score: 0.4396)\n  - base_model.model.model.layers.2.mlp.down_proj: r=8 (Unchanged, Score: 0.4396)\n  - base_model.model.model.layers.3.self_attn.q_proj: r=8 â†’ 12 (Score: 0.7660)\n  - base_model.model.model.layers.3.self_attn.k_proj: r=8 â†’ 10 (Score: 0.5825)\n  - base_model.model.model.layers.3.self_attn.v_proj: r=8 (Unchanged, Score: 0.4340)\n  - base_model.model.model.layers.3.self_attn.o_proj: r=8 (Unchanged, Score: 0.3752)\n  - base_model.model.model.layers.3.mlp.gate_proj: r=8 â†’ 9 (Score: 0.4855)\n  - base_model.model.model.layers.3.mlp.up_proj: r=8 â†’ 9 (Score: 0.4878)\n  - base_model.model.model.layers.3.mlp.down_proj: r=8 â†’ 9 (Score: 0.4879)\n  - base_model.model.model.layers.4.self_attn.q_proj: r=8 â†’ 11 (Score: 0.6809)\n  - base_model.model.model.layers.4.self_attn.k_proj: r=8 (Unchanged, Score: 0.4528)\n  - base_model.model.model.layers.4.self_attn.v_proj: r=8 â†’ 7 (Score: 0.3067)\n  - base_model.model.model.layers.4.self_attn.o_proj: r=8 â†’ 7 (Score: 0.2531)\n  - base_model.model.model.layers.4.mlp.gate_proj: r=8 â†’ 9 (Score: 0.4838)\n  - base_model.model.model.layers.4.mlp.up_proj: r=8 (Unchanged, Score: 0.4272)\n  - base_model.model.model.layers.4.mlp.down_proj: r=8 (Unchanged, Score: 0.4273)\n  - base_model.model.model.layers.5.self_attn.q_proj: r=8 â†’ 11 (Score: 0.7410)\n  - base_model.model.model.layers.5.self_attn.k_proj: r=8 â†’ 10 (Score: 0.5975)\n  - base_model.model.model.layers.5.self_attn.v_proj: r=8 (Unchanged, Score: 0.4625)\n  - base_model.model.model.layers.5.self_attn.o_proj: r=8 (Unchanged, Score: 0.3947)\n  - base_model.model.model.layers.5.mlp.gate_proj: r=8 (Unchanged, Score: 0.4699)\n  - base_model.model.model.layers.5.mlp.up_proj: r=8 (Unchanged, Score: 0.4691)\n  - base_model.model.model.layers.5.mlp.down_proj: r=8 (Unchanged, Score: 0.4691)\n  - base_model.model.model.layers.6.self_attn.q_proj: r=8 â†’ 13 (Score: 0.8334)\n  - base_model.model.model.layers.6.self_attn.k_proj: r=8 â†’ 9 (Score: 0.4855)\n  - base_model.model.model.layers.6.self_attn.v_proj: r=8 (Unchanged, Score: 0.4182)\n  - base_model.model.model.layers.6.self_attn.o_proj: r=8 (Unchanged, Score: 0.4062)\n  - base_model.model.model.layers.6.mlp.gate_proj: r=8 â†’ 9 (Score: 0.5470)\n  - base_model.model.model.layers.6.mlp.up_proj: r=8 â†’ 9 (Score: 0.5514)\n  - base_model.model.model.layers.6.mlp.down_proj: r=8 â†’ 9 (Score: 0.5513)\n  - base_model.model.model.layers.7.self_attn.q_proj: r=8 â†’ 10 (Score: 0.6668)\n  - base_model.model.model.layers.7.self_attn.k_proj: r=8 â†’ 9 (Score: 0.5332)\n  - base_model.model.model.layers.7.self_attn.v_proj: r=8 (Unchanged, Score: 0.4479)\n  - base_model.model.model.layers.7.self_attn.o_proj: r=8 â†’ 7 (Score: 0.3448)\n  - base_model.model.model.layers.7.mlp.gate_proj: r=8 â†’ 11 (Score: 0.7218)\n  - base_model.model.model.layers.7.mlp.up_proj: r=8 â†’ 10 (Score: 0.5997)\n  - base_model.model.model.layers.7.mlp.down_proj: r=8 â†’ 10 (Score: 0.6000)\n  - base_model.model.model.layers.8.self_attn.q_proj: r=8 â†’ 11 (Score: 0.7064)\n  - base_model.model.model.layers.8.self_attn.k_proj: r=8 â†’ 9 (Score: 0.5176)\n  - base_model.model.model.layers.8.self_attn.v_proj: r=8 (Unchanged, Score: 0.4220)\n  - base_model.model.model.layers.8.self_attn.o_proj: r=8 â†’ 7 (Score: 0.2562)\n  - base_model.model.model.layers.8.mlp.gate_proj: r=8 â†’ 11 (Score: 0.6858)\n  - base_model.model.model.layers.8.mlp.up_proj: r=8 â†’ 9 (Score: 0.5222)\n  - base_model.model.model.layers.8.mlp.down_proj: r=8 â†’ 9 (Score: 0.5220)\n  - base_model.model.model.layers.9.self_attn.q_proj: r=8 â†’ 9 (Score: 0.5352)\n  - base_model.model.model.layers.9.self_attn.k_proj: r=8 â†’ 9 (Score: 0.5435)\n  - base_model.model.model.layers.9.self_attn.v_proj: r=8 (Unchanged, Score: 0.3765)\n  - base_model.model.model.layers.9.self_attn.o_proj: r=8 â†’ 7 (Score: 0.3462)\n  - base_model.model.model.layers.9.mlp.gate_proj: r=8 â†’ 10 (Score: 0.6137)\n  - base_model.model.model.layers.9.mlp.up_proj: r=8 â†’ 10 (Score: 0.5872)\n  - base_model.model.model.layers.9.mlp.down_proj: r=8 â†’ 10 (Score: 0.5870)\n  - base_model.model.model.layers.10.self_attn.q_proj: r=8 â†’ 9 (Score: 0.4903)\n  - base_model.model.model.layers.10.self_attn.k_proj: r=8 (Unchanged, Score: 0.3977)\n  - base_model.model.model.layers.10.self_attn.v_proj: r=8 (Unchanged, Score: 0.3961)\n  - base_model.model.model.layers.10.self_attn.o_proj: r=8 â†’ 7 (Score: 0.3425)\n  - base_model.model.model.layers.10.mlp.gate_proj: r=8 â†’ 9 (Score: 0.5251)\n  - base_model.model.model.layers.10.mlp.up_proj: r=8 â†’ 9 (Score: 0.4969)\n  - base_model.model.model.layers.10.mlp.down_proj: r=8 â†’ 9 (Score: 0.4968)\n  - base_model.model.model.layers.11.self_attn.q_proj: r=8 (Unchanged, Score: 0.4435)\n  - base_model.model.model.layers.11.self_attn.k_proj: r=8 â†’ 11 (Score: 0.7283)\n  - base_model.model.model.layers.11.self_attn.v_proj: r=8 (Unchanged, Score: 0.4465)\n  - base_model.model.model.layers.11.self_attn.o_proj: r=8 â†’ 7 (Score: 0.2825)\n  - base_model.model.model.layers.11.mlp.gate_proj: r=8 â†’ 9 (Score: 0.5380)\n  - base_model.model.model.layers.11.mlp.up_proj: r=8 â†’ 9 (Score: 0.5755)\n  - base_model.model.model.layers.11.mlp.down_proj: r=8 â†’ 9 (Score: 0.5753)\n  - base_model.model.model.layers.12.self_attn.q_proj: r=8 â†’ 7 (Score: 0.3318)\n  - base_model.model.model.layers.12.self_attn.k_proj: r=8 â†’ 10 (Score: 0.6259)\n  - base_model.model.model.layers.12.self_attn.v_proj: r=8 â†’ 7 (Score: 0.3685)\n  - base_model.model.model.layers.12.self_attn.o_proj: r=8 â†’ 7 (Score: 0.2490)\n  - base_model.model.model.layers.12.mlp.gate_proj: r=8 (Unchanged, Score: 0.4022)\n  - base_model.model.model.layers.12.mlp.up_proj: r=8 (Unchanged, Score: 0.3805)\n  - base_model.model.model.layers.12.mlp.down_proj: r=8 (Unchanged, Score: 0.3805)\n  - base_model.model.model.layers.13.self_attn.q_proj: r=8 â†’ 6 (Score: 0.2295)\n  - base_model.model.model.layers.13.self_attn.k_proj: r=8 (Unchanged, Score: 0.3981)\n  - base_model.model.model.layers.13.self_attn.v_proj: r=8 â†’ 7 (Score: 0.2741)\n  - base_model.model.model.layers.13.self_attn.o_proj: r=8 â†’ 6 (Score: 0.1901)\n  - base_model.model.model.layers.13.mlp.gate_proj: r=8 â†’ 9 (Score: 0.5003)\n  - base_model.model.model.layers.13.mlp.up_proj: r=8 (Unchanged, Score: 0.4018)\n  - base_model.model.model.layers.13.mlp.down_proj: r=8 (Unchanged, Score: 0.4020)\n  - base_model.model.model.layers.14.self_attn.q_proj: r=8 â†’ 6 (Score: 0.2099)\n  - base_model.model.model.layers.14.self_attn.k_proj: r=8 (Unchanged, Score: 0.3763)\n  - base_model.model.model.layers.14.self_attn.v_proj: r=8 â†’ 7 (Score: 0.2846)\n  - base_model.model.model.layers.14.self_attn.o_proj: r=8 â†’ 7 (Score: 0.2575)\n  - base_model.model.model.layers.14.mlp.gate_proj: r=8 (Unchanged, Score: 0.4407)\n  - base_model.model.model.layers.14.mlp.up_proj: r=8 â†’ 7 (Score: 0.3425)\n  - base_model.model.model.layers.14.mlp.down_proj: r=8 â†’ 7 (Score: 0.3426)\n  - base_model.model.model.layers.15.self_attn.q_proj: r=8 â†’ 6 (Score: 0.1388)\n  - base_model.model.model.layers.15.self_attn.k_proj: r=8 (Unchanged, Score: 0.4303)\n  - base_model.model.model.layers.15.self_attn.v_proj: r=8 â†’ 7 (Score: 0.2463)\n  - base_model.model.model.layers.15.self_attn.o_proj: r=8 â†’ 6 (Score: 0.1289)\n  - base_model.model.model.layers.15.mlp.gate_proj: r=8 â†’ 7 (Score: 0.3402)\n  - base_model.model.model.layers.15.mlp.up_proj: r=8 â†’ 7 (Score: 0.3113)\n  - base_model.model.model.layers.15.mlp.down_proj: r=8 â†’ 7 (Score: 0.3112)\n  - base_model.model.model.layers.16.self_attn.q_proj: r=8 â†’ 6 (Score: 0.1464)\n  - base_model.model.model.layers.16.self_attn.k_proj: r=8 â†’ 7 (Score: 0.3487)\n  - base_model.model.model.layers.16.self_attn.v_proj: r=8 â†’ 7 (Score: 0.2669)\n  - base_model.model.model.layers.16.self_attn.o_proj: r=8 â†’ 6 (Score: 0.1319)\n  - base_model.model.model.layers.16.mlp.gate_proj: r=8 (Unchanged, Score: 0.3841)\n  - base_model.model.model.layers.16.mlp.up_proj: r=8 â†’ 7 (Score: 0.3103)\n  - base_model.model.model.layers.16.mlp.down_proj: r=8 â†’ 7 (Score: 0.3102)\n  - base_model.model.model.layers.17.self_attn.q_proj: r=8 â†’ 6 (Score: 0.1836)\n  - base_model.model.model.layers.17.self_attn.k_proj: r=8 (Unchanged, Score: 0.4692)\n  - base_model.model.model.layers.17.self_attn.v_proj: r=8 â†’ 7 (Score: 0.2877)\n  - base_model.model.model.layers.17.self_attn.o_proj: r=8 â†’ 6 (Score: 0.1120)\n  - base_model.model.model.layers.17.mlp.gate_proj: r=8 (Unchanged, Score: 0.3773)\n  - base_model.model.model.layers.17.mlp.up_proj: r=8 â†’ 7 (Score: 0.3122)\n  - base_model.model.model.layers.17.mlp.down_proj: r=8 â†’ 7 (Score: 0.3123)\n  - base_model.model.model.layers.18.self_attn.q_proj: r=8 â†’ 6 (Score: 0.1155)\n  - base_model.model.model.layers.18.self_attn.k_proj: r=8 (Unchanged, Score: 0.3825)\n  - base_model.model.model.layers.18.self_attn.v_proj: r=8 â†’ 7 (Score: 0.2494)\n  - base_model.model.model.layers.18.self_attn.o_proj: r=8 â†’ 5 (Score: 0.0649)\n  - base_model.model.model.layers.18.mlp.gate_proj: r=8 â†’ 7 (Score: 0.3426)\n  - base_model.model.model.layers.18.mlp.up_proj: r=8 â†’ 7 (Score: 0.2515)\n  - base_model.model.model.layers.18.mlp.down_proj: r=8 â†’ 7 (Score: 0.2515)\n  - base_model.model.model.layers.19.self_attn.q_proj: r=8 â†’ 6 (Score: 0.0985)\n  - base_model.model.model.layers.19.self_attn.k_proj: r=8 (Unchanged, Score: 0.4211)\n  - base_model.model.model.layers.19.self_attn.v_proj: r=8 (Unchanged, Score: 0.3722)\n  - base_model.model.model.layers.19.self_attn.o_proj: r=8 â†’ 5 (Score: 0.0574)\n  - base_model.model.model.layers.19.mlp.gate_proj: r=8 â†’ 7 (Score: 0.3293)\n  - base_model.model.model.layers.19.mlp.up_proj: r=8 â†’ 7 (Score: 0.2615)\n  - base_model.model.model.layers.19.mlp.down_proj: r=8 â†’ 7 (Score: 0.2616)\n  - base_model.model.model.layers.20.self_attn.q_proj: r=8 â†’ 5 (Score: 0.0594)\n  - base_model.model.model.layers.20.self_attn.k_proj: r=8 (Unchanged, Score: 0.4452)\n  - base_model.model.model.layers.20.self_attn.v_proj: r=8 â†’ 7 (Score: 0.2661)\n  - base_model.model.model.layers.20.self_attn.o_proj: r=8 â†’ 5 (Score: 0.0481)\n  - base_model.model.model.layers.20.mlp.gate_proj: r=8 (Unchanged, Score: 0.4332)\n  - base_model.model.model.layers.20.mlp.up_proj: r=8 â†’ 7 (Score: 0.2673)\n  - base_model.model.model.layers.20.mlp.down_proj: r=8 â†’ 7 (Score: 0.2673)\n  - base_model.model.model.layers.21.self_attn.q_proj: r=8 â†’ 5 (Score: 0.0052)\n  - base_model.model.model.layers.21.self_attn.k_proj: r=8 (Unchanged, Score: 0.4380)\n  - base_model.model.model.layers.21.self_attn.v_proj: r=8 â†’ 7 (Score: 0.2597)\n  - base_model.model.model.layers.21.self_attn.o_proj: r=8 â†’ 5 (Score: 0.0000)\n  - base_model.model.model.layers.21.mlp.gate_proj: r=8 â†’ 6 (Score: 0.1435)\n  - base_model.model.model.layers.21.mlp.up_proj: r=8 â†’ 6 (Score: 0.1483)\n  - base_model.model.model.layers.21.mlp.down_proj: r=8 â†’ 6 (Score: 0.1483)\nâœ… AdaptiveLoRA: Rank setup for Epoch 1 complete.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 2:23:54, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.415200</td>\n      <td>0.392715</td>\n      <td>0.830667</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.401300</td>\n      <td>0.411046</td>\n      <td>0.812000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.408600</td>\n      <td>0.408037</td>\n      <td>0.820000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ðŸ“„ Epoch 1: Rank allocations logged to ./logs/adaptive_lora_epoch_logs.csv\n\n\n--- AdaptiveLoRA: Preparing ranks for Epoch 2 ---\nComputing BI importance scores (pre-training)...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing Importance:   0%|          | 0/47 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Allocating new ranks based on BI scores...\nApplying new ranks to LoRA modules for this epoch...\n  - base_model.model.model.layers.0.self_attn.q_proj: r=6 â†’ 7 (Score: 0.2362)\n  - base_model.model.model.layers.0.self_attn.k_proj: r=9 â†’ 8 (Score: 0.4294)\n  - base_model.model.model.layers.0.self_attn.v_proj: r=7 (Unchanged, Score: 0.3175)\n  - base_model.model.model.layers.0.self_attn.o_proj: r=11 â†’ 10 (Score: 0.5648)\n  - base_model.model.model.layers.0.mlp.gate_proj: r=8 (Unchanged, Score: 0.4285)\n  - base_model.model.model.layers.0.mlp.up_proj: r=8 â†’ 9 (Score: 0.4561)\n  - base_model.model.model.layers.0.mlp.down_proj: r=8 â†’ 9 (Score: 0.4561)\n  - base_model.model.model.layers.1.self_attn.q_proj: r=7 â†’ 8 (Score: 0.3515)\n  - base_model.model.model.layers.1.self_attn.k_proj: r=15 â†’ 13 (Score: 0.8518)\n  - base_model.model.model.layers.1.self_attn.v_proj: r=9 â†’ 8 (Score: 0.4226)\n  - base_model.model.model.layers.1.self_attn.o_proj: r=8 (Unchanged, Score: 0.4016)\n  - base_model.model.model.layers.1.mlp.gate_proj: r=9 â†’ 8 (Score: 0.4031)\n  - base_model.model.model.layers.1.mlp.up_proj: r=8 (Unchanged, Score: 0.3893)\n  - base_model.model.model.layers.1.mlp.down_proj: r=8 (Unchanged, Score: 0.3895)\n  - base_model.model.model.layers.2.self_attn.q_proj: r=8 (Unchanged, Score: 0.3640)\n  - base_model.model.model.layers.2.self_attn.k_proj: r=14 (Unchanged, Score: 0.8859)\n  - base_model.model.model.layers.2.self_attn.v_proj: r=9 (Unchanged, Score: 0.4702)\n  - base_model.model.model.layers.2.self_attn.o_proj: r=8 (Unchanged, Score: 0.4312)\n  - base_model.model.model.layers.2.mlp.gate_proj: r=8 â†’ 9 (Score: 0.4697)\n  - base_model.model.model.layers.2.mlp.up_proj: r=8 â†’ 9 (Score: 0.4684)\n  - base_model.model.model.layers.2.mlp.down_proj: r=8 â†’ 9 (Score: 0.4683)\n  - base_model.model.model.layers.3.self_attn.q_proj: r=12 (Unchanged, Score: 0.7612)\n  - base_model.model.model.layers.3.self_attn.k_proj: r=10 â†’ 9 (Score: 0.4684)\n  - base_model.model.model.layers.3.self_attn.v_proj: r=8 (Unchanged, Score: 0.3871)\n  - base_model.model.model.layers.3.self_attn.o_proj: r=8 â†’ 7 (Score: 0.3269)\n  - base_model.model.model.layers.3.mlp.gate_proj: r=9 (Unchanged, Score: 0.4488)\n  - base_model.model.model.layers.3.mlp.up_proj: r=9 (Unchanged, Score: 0.4423)\n  - base_model.model.model.layers.3.mlp.down_proj: r=9 (Unchanged, Score: 0.4423)\n  - base_model.model.model.layers.4.self_attn.q_proj: r=11 (Unchanged, Score: 0.6585)\n  - base_model.model.model.layers.4.self_attn.k_proj: r=8 (Unchanged, Score: 0.3903)\n  - base_model.model.model.layers.4.self_attn.v_proj: r=7 (Unchanged, Score: 0.3260)\n  - base_model.model.model.layers.4.self_attn.o_proj: r=7 (Unchanged, Score: 0.3183)\n  - base_model.model.model.layers.4.mlp.gate_proj: r=9 (Unchanged, Score: 0.4639)\n  - base_model.model.model.layers.4.mlp.up_proj: r=8 (Unchanged, Score: 0.4226)\n  - base_model.model.model.layers.4.mlp.down_proj: r=8 (Unchanged, Score: 0.4226)\n  - base_model.model.model.layers.5.self_attn.q_proj: r=11 â†’ 12 (Score: 0.7781)\n  - base_model.model.model.layers.5.self_attn.k_proj: r=10 â†’ 9 (Score: 0.5225)\n  - base_model.model.model.layers.5.self_attn.v_proj: r=8 (Unchanged, Score: 0.3838)\n  - base_model.model.model.layers.5.self_attn.o_proj: r=8 (Unchanged, Score: 0.3654)\n  - base_model.model.model.layers.5.mlp.gate_proj: r=8 â†’ 9 (Score: 0.5247)\n  - base_model.model.model.layers.5.mlp.up_proj: r=8 â†’ 9 (Score: 0.5364)\n  - base_model.model.model.layers.5.mlp.down_proj: r=8 â†’ 9 (Score: 0.5363)\n  - base_model.model.model.layers.6.self_attn.q_proj: r=13 (Unchanged, Score: 0.8261)\n  - base_model.model.model.layers.6.self_attn.k_proj: r=9 (Unchanged, Score: 0.5302)\n  - base_model.model.model.layers.6.self_attn.v_proj: r=8 â†’ 9 (Score: 0.4547)\n  - base_model.model.model.layers.6.self_attn.o_proj: r=8 (Unchanged, Score: 0.4155)\n  - base_model.model.model.layers.6.mlp.gate_proj: r=9 â†’ 10 (Score: 0.5453)\n  - base_model.model.model.layers.6.mlp.up_proj: r=9 â†’ 10 (Score: 0.5870)\n  - base_model.model.model.layers.6.mlp.down_proj: r=9 â†’ 10 (Score: 0.5870)\n  - base_model.model.model.layers.7.self_attn.q_proj: r=10 â†’ 12 (Score: 0.7386)\n  - base_model.model.model.layers.7.self_attn.k_proj: r=9 â†’ 10 (Score: 0.5803)\n  - base_model.model.model.layers.7.self_attn.v_proj: r=8 â†’ 9 (Score: 0.4914)\n  - base_model.model.model.layers.7.self_attn.o_proj: r=7 â†’ 9 (Score: 0.5130)\n  - base_model.model.model.layers.7.mlp.gate_proj: r=11 â†’ 16 (Score: 0.9901)\n  - base_model.model.model.layers.7.mlp.up_proj: r=10 â†’ 12 (Score: 0.7493)\n  - base_model.model.model.layers.7.mlp.down_proj: r=10 â†’ 12 (Score: 0.7493)\n  - base_model.model.model.layers.8.self_attn.q_proj: r=11 â†’ 12 (Score: 0.7804)\n  - base_model.model.model.layers.8.self_attn.k_proj: r=9 â†’ 11 (Score: 0.6618)\n  - base_model.model.model.layers.8.self_attn.v_proj: r=8 â†’ 10 (Score: 0.5607)\n  - base_model.model.model.layers.8.self_attn.o_proj: r=7 â†’ 8 (Score: 0.3807)\n  - base_model.model.model.layers.8.mlp.gate_proj: r=11 â†’ 13 (Score: 0.8119)\n  - base_model.model.model.layers.8.mlp.up_proj: r=9 â†’ 11 (Score: 0.6688)\n  - base_model.model.model.layers.8.mlp.down_proj: r=9 â†’ 11 (Score: 0.6688)\n  - base_model.model.model.layers.9.self_attn.q_proj: r=9 (Unchanged, Score: 0.5284)\n  - base_model.model.model.layers.9.self_attn.k_proj: r=9 â†’ 11 (Score: 0.6786)\n  - base_model.model.model.layers.9.self_attn.v_proj: r=8 â†’ 11 (Score: 0.6407)\n  - base_model.model.model.layers.9.self_attn.o_proj: r=7 â†’ 8 (Score: 0.3641)\n  - base_model.model.model.layers.9.mlp.gate_proj: r=10 â†’ 11 (Score: 0.6744)\n  - base_model.model.model.layers.9.mlp.up_proj: r=10 (Unchanged, Score: 0.5911)\n  - base_model.model.model.layers.9.mlp.down_proj: r=10 (Unchanged, Score: 0.5908)\n  - base_model.model.model.layers.10.self_attn.q_proj: r=9 (Unchanged, Score: 0.5174)\n  - base_model.model.model.layers.10.self_attn.k_proj: r=8 â†’ 11 (Score: 0.7000)\n  - base_model.model.model.layers.10.self_attn.v_proj: r=8 â†’ 9 (Score: 0.4545)\n  - base_model.model.model.layers.10.self_attn.o_proj: r=7 â†’ 9 (Score: 0.4415)\n  - base_model.model.model.layers.10.mlp.gate_proj: r=9 â†’ 11 (Score: 0.7099)\n  - base_model.model.model.layers.10.mlp.up_proj: r=9 â†’ 10 (Score: 0.5580)\n  - base_model.model.model.layers.10.mlp.down_proj: r=9 â†’ 10 (Score: 0.5581)\n  - base_model.model.model.layers.11.self_attn.q_proj: r=8 (Unchanged, Score: 0.3521)\n  - base_model.model.model.layers.11.self_attn.k_proj: r=11 â†’ 16 (Score: 1.0000)\n  - base_model.model.model.layers.11.self_attn.v_proj: r=8 â†’ 10 (Score: 0.6048)\n  - base_model.model.model.layers.11.self_attn.o_proj: r=7 â†’ 9 (Score: 0.4908)\n  - base_model.model.model.layers.11.mlp.gate_proj: r=9 â†’ 8 (Score: 0.3586)\n  - base_model.model.model.layers.11.mlp.up_proj: r=9 â†’ 8 (Score: 0.3928)\n  - base_model.model.model.layers.11.mlp.down_proj: r=9 â†’ 8 (Score: 0.3929)\n  - base_model.model.model.layers.12.self_attn.q_proj: r=7 (Unchanged, Score: 0.1984)\n  - base_model.model.model.layers.12.self_attn.k_proj: r=10 (Unchanged, Score: 0.6260)\n  - base_model.model.model.layers.12.self_attn.v_proj: r=7 â†’ 9 (Score: 0.4897)\n  - base_model.model.model.layers.12.self_attn.o_proj: r=7 â†’ 6 (Score: 0.1752)\n  - base_model.model.model.layers.12.mlp.gate_proj: r=8 â†’ 7 (Score: 0.2209)\n  - base_model.model.model.layers.12.mlp.up_proj: r=8 â†’ 7 (Score: 0.2451)\n  - base_model.model.model.layers.12.mlp.down_proj: r=8 â†’ 7 (Score: 0.2451)\n  - base_model.model.model.layers.13.self_attn.q_proj: r=6 (Unchanged, Score: 0.1708)\n  - base_model.model.model.layers.13.self_attn.k_proj: r=8 (Unchanged, Score: 0.3954)\n  - base_model.model.model.layers.13.self_attn.v_proj: r=7 (Unchanged, Score: 0.2746)\n  - base_model.model.model.layers.13.self_attn.o_proj: r=6 (Unchanged, Score: 0.1332)\n  - base_model.model.model.layers.13.mlp.gate_proj: r=9 â†’ 7 (Score: 0.2836)\n  - base_model.model.model.layers.13.mlp.up_proj: r=8 â†’ 7 (Score: 0.2761)\n  - base_model.model.model.layers.13.mlp.down_proj: r=8 â†’ 7 (Score: 0.2759)\n  - base_model.model.model.layers.14.self_attn.q_proj: r=6 (Unchanged, Score: 0.1293)\n  - base_model.model.model.layers.14.self_attn.k_proj: r=8 â†’ 7 (Score: 0.2549)\n  - base_model.model.model.layers.14.self_attn.v_proj: r=7 â†’ 6 (Score: 0.1823)\n  - base_model.model.model.layers.14.self_attn.o_proj: r=7 â†’ 6 (Score: 0.1111)\n  - base_model.model.model.layers.14.mlp.gate_proj: r=8 â†’ 7 (Score: 0.2157)\n  - base_model.model.model.layers.14.mlp.up_proj: r=7 â†’ 6 (Score: 0.1490)\n  - base_model.model.model.layers.14.mlp.down_proj: r=7 â†’ 6 (Score: 0.1490)\n  - base_model.model.model.layers.15.self_attn.q_proj: r=6 (Unchanged, Score: 0.0820)\n  - base_model.model.model.layers.15.self_attn.k_proj: r=8 â†’ 6 (Score: 0.1409)\n  - base_model.model.model.layers.15.self_attn.v_proj: r=7 â†’ 6 (Score: 0.0994)\n  - base_model.model.model.layers.15.self_attn.o_proj: r=6 (Unchanged, Score: 0.0575)\n  - base_model.model.model.layers.15.mlp.gate_proj: r=7 â†’ 6 (Score: 0.1383)\n  - base_model.model.model.layers.15.mlp.up_proj: r=7 â†’ 6 (Score: 0.1066)\n  - base_model.model.model.layers.15.mlp.down_proj: r=7 â†’ 6 (Score: 0.1067)\n  - base_model.model.model.layers.16.self_attn.q_proj: r=6 (Unchanged, Score: 0.0631)\n  - base_model.model.model.layers.16.self_attn.k_proj: r=7 â†’ 6 (Score: 0.1924)\n  - base_model.model.model.layers.16.self_attn.v_proj: r=7 â†’ 6 (Score: 0.1784)\n  - base_model.model.model.layers.16.self_attn.o_proj: r=6 (Unchanged, Score: 0.0963)\n  - base_model.model.model.layers.16.mlp.gate_proj: r=8 â†’ 6 (Score: 0.1526)\n  - base_model.model.model.layers.16.mlp.up_proj: r=7 â†’ 6 (Score: 0.1040)\n  - base_model.model.model.layers.16.mlp.down_proj: r=7 â†’ 6 (Score: 0.1040)\n  - base_model.model.model.layers.17.self_attn.q_proj: r=6 (Unchanged, Score: 0.0699)\n  - base_model.model.model.layers.17.self_attn.k_proj: r=8 â†’ 6 (Score: 0.1018)\n  - base_model.model.model.layers.17.self_attn.v_proj: r=7 â†’ 6 (Score: 0.0645)\n  - base_model.model.model.layers.17.self_attn.o_proj: r=6 â†’ 5 (Score: 0.0317)\n  - base_model.model.model.layers.17.mlp.gate_proj: r=8 â†’ 6 (Score: 0.1598)\n  - base_model.model.model.layers.17.mlp.up_proj: r=7 â†’ 6 (Score: 0.1263)\n  - base_model.model.model.layers.17.mlp.down_proj: r=7 â†’ 6 (Score: 0.1263)\n  - base_model.model.model.layers.18.self_attn.q_proj: r=6 â†’ 5 (Score: 0.0285)\n  - base_model.model.model.layers.18.self_attn.k_proj: r=8 â†’ 6 (Score: 0.1688)\n  - base_model.model.model.layers.18.self_attn.v_proj: r=7 â†’ 6 (Score: 0.1568)\n  - base_model.model.model.layers.18.self_attn.o_proj: r=5 (Unchanged, Score: 0.0452)\n  - base_model.model.model.layers.18.mlp.gate_proj: r=7 â†’ 6 (Score: 0.1288)\n  - base_model.model.model.layers.18.mlp.up_proj: r=7 â†’ 6 (Score: 0.1342)\n  - base_model.model.model.layers.18.mlp.down_proj: r=7 â†’ 6 (Score: 0.1341)\n  - base_model.model.model.layers.19.self_attn.q_proj: r=6 â†’ 5 (Score: 0.0290)\n  - base_model.model.model.layers.19.self_attn.k_proj: r=8 â†’ 6 (Score: 0.0816)\n  - base_model.model.model.layers.19.self_attn.v_proj: r=8 â†’ 6 (Score: 0.1021)\n  - base_model.model.model.layers.19.self_attn.o_proj: r=5 (Unchanged, Score: 0.0379)\n  - base_model.model.model.layers.19.mlp.gate_proj: r=7 â†’ 6 (Score: 0.1117)\n  - base_model.model.model.layers.19.mlp.up_proj: r=7 â†’ 6 (Score: 0.0871)\n  - base_model.model.model.layers.19.mlp.down_proj: r=7 â†’ 6 (Score: 0.0871)\n  - base_model.model.model.layers.20.self_attn.q_proj: r=5 â†’ 6 (Score: 0.1008)\n  - base_model.model.model.layers.20.self_attn.k_proj: r=8 â†’ 6 (Score: 0.1359)\n  - base_model.model.model.layers.20.self_attn.v_proj: r=7 â†’ 5 (Score: 0.0426)\n  - base_model.model.model.layers.20.self_attn.o_proj: r=5 (Unchanged, Score: 0.0066)\n  - base_model.model.model.layers.20.mlp.gate_proj: r=8 â†’ 7 (Score: 0.3202)\n  - base_model.model.model.layers.20.mlp.up_proj: r=7 â†’ 6 (Score: 0.1748)\n  - base_model.model.model.layers.20.mlp.down_proj: r=7 â†’ 6 (Score: 0.1748)\n  - base_model.model.model.layers.21.self_attn.q_proj: r=5 (Unchanged, Score: 0.0000)\n  - base_model.model.model.layers.21.self_attn.k_proj: r=8 â†’ 6 (Score: 0.1377)\n  - base_model.model.model.layers.21.self_attn.v_proj: r=7 â†’ 6 (Score: 0.0946)\n  - base_model.model.model.layers.21.self_attn.o_proj: r=5 (Unchanged, Score: 0.0181)\n  - base_model.model.model.layers.21.mlp.gate_proj: r=6 (Unchanged, Score: 0.1290)\n  - base_model.model.model.layers.21.mlp.up_proj: r=6 (Unchanged, Score: 0.1414)\n  - base_model.model.model.layers.21.mlp.down_proj: r=6 (Unchanged, Score: 0.1413)\nâœ… AdaptiveLoRA: Rank setup for Epoch 2 complete.\n\nðŸ“„ Epoch 2: Rank allocations logged to ./logs/adaptive_lora_epoch_logs.csv\n\n\n--- AdaptiveLoRA: Preparing ranks for Epoch 3 ---\nComputing BI importance scores (pre-training)...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing Importance:   0%|          | 0/47 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Allocating new ranks based on BI scores...\nApplying new ranks to LoRA modules for this epoch...\n  - base_model.model.model.layers.0.self_attn.q_proj: r=7 (Unchanged, Score: 0.3163)\n  - base_model.model.model.layers.0.self_attn.k_proj: r=8 â†’ 9 (Score: 0.5673)\n  - base_model.model.model.layers.0.self_attn.v_proj: r=7 â†’ 8 (Score: 0.4278)\n  - base_model.model.model.layers.0.self_attn.o_proj: r=10 â†’ 11 (Score: 0.6781)\n  - base_model.model.model.layers.0.mlp.gate_proj: r=8 â†’ 9 (Score: 0.5537)\n  - base_model.model.model.layers.0.mlp.up_proj: r=9 (Unchanged, Score: 0.5705)\n  - base_model.model.model.layers.0.mlp.down_proj: r=9 (Unchanged, Score: 0.5704)\n  - base_model.model.model.layers.1.self_attn.q_proj: r=8 (Unchanged, Score: 0.4607)\n  - base_model.model.model.layers.1.self_attn.k_proj: r=13 â†’ 15 (Score: 0.9799)\n  - base_model.model.model.layers.1.self_attn.v_proj: r=8 â†’ 9 (Score: 0.5041)\n  - base_model.model.model.layers.1.self_attn.o_proj: r=8 â†’ 9 (Score: 0.5054)\n  - base_model.model.model.layers.1.mlp.gate_proj: r=8 (Unchanged, Score: 0.4204)\n  - base_model.model.model.layers.1.mlp.up_proj: r=8 (Unchanged, Score: 0.4001)\n  - base_model.model.model.layers.1.mlp.down_proj: r=8 (Unchanged, Score: 0.4000)\n  - base_model.model.model.layers.2.self_attn.q_proj: r=8 (Unchanged, Score: 0.4366)\n  - base_model.model.model.layers.2.self_attn.k_proj: r=14 â†’ 15 (Score: 1.0000)\n  - base_model.model.model.layers.2.self_attn.v_proj: r=9 (Unchanged, Score: 0.4914)\n  - base_model.model.model.layers.2.self_attn.o_proj: r=8 (Unchanged, Score: 0.4662)\n  - base_model.model.model.layers.2.mlp.gate_proj: r=9 (Unchanged, Score: 0.5015)\n  - base_model.model.model.layers.2.mlp.up_proj: r=9 (Unchanged, Score: 0.5140)\n  - base_model.model.model.layers.2.mlp.down_proj: r=9 (Unchanged, Score: 0.5141)\n  - base_model.model.model.layers.3.self_attn.q_proj: r=12 â†’ 13 (Score: 0.8403)\n  - base_model.model.model.layers.3.self_attn.k_proj: r=9 â†’ 10 (Score: 0.5989)\n  - base_model.model.model.layers.3.self_attn.v_proj: r=8 (Unchanged, Score: 0.4452)\n  - base_model.model.model.layers.3.self_attn.o_proj: r=7 (Unchanged, Score: 0.3615)\n  - base_model.model.model.layers.3.mlp.gate_proj: r=9 (Unchanged, Score: 0.5724)\n  - base_model.model.model.layers.3.mlp.up_proj: r=9 (Unchanged, Score: 0.5673)\n  - base_model.model.model.layers.3.mlp.down_proj: r=9 (Unchanged, Score: 0.5674)\n  - base_model.model.model.layers.4.self_attn.q_proj: r=11 â†’ 13 (Score: 0.8238)\n  - base_model.model.model.layers.4.self_attn.k_proj: r=8 â†’ 9 (Score: 0.5253)\n  - base_model.model.model.layers.4.self_attn.v_proj: r=7 â†’ 8 (Score: 0.4041)\n  - base_model.model.model.layers.4.self_attn.o_proj: r=7 â†’ 8 (Score: 0.3901)\n  - base_model.model.model.layers.4.mlp.gate_proj: r=9 (Unchanged, Score: 0.5613)\n  - base_model.model.model.layers.4.mlp.up_proj: r=8 â†’ 9 (Score: 0.5535)\n  - base_model.model.model.layers.4.mlp.down_proj: r=8 â†’ 9 (Score: 0.5532)\n  - base_model.model.model.layers.5.self_attn.q_proj: r=12 (Unchanged, Score: 0.8116)\n  - base_model.model.model.layers.5.self_attn.k_proj: r=9 â†’ 11 (Score: 0.7377)\n  - base_model.model.model.layers.5.self_attn.v_proj: r=8 (Unchanged, Score: 0.4707)\n  - base_model.model.model.layers.5.self_attn.o_proj: r=8 â†’ 9 (Score: 0.5602)\n  - base_model.model.model.layers.5.mlp.gate_proj: r=9 â†’ 11 (Score: 0.6736)\n  - base_model.model.model.layers.5.mlp.up_proj: r=9 â†’ 11 (Score: 0.7049)\n  - base_model.model.model.layers.5.mlp.down_proj: r=9 â†’ 11 (Score: 0.7046)\n  - base_model.model.model.layers.6.self_attn.q_proj: r=13 â†’ 12 (Score: 0.7485)\n  - base_model.model.model.layers.6.self_attn.k_proj: r=9 (Unchanged, Score: 0.5377)\n  - base_model.model.model.layers.6.self_attn.v_proj: r=9 â†’ 8 (Score: 0.4586)\n  - base_model.model.model.layers.6.self_attn.o_proj: r=8 (Unchanged, Score: 0.4237)\n  - base_model.model.model.layers.6.mlp.gate_proj: r=10 (Unchanged, Score: 0.6125)\n  - base_model.model.model.layers.6.mlp.up_proj: r=10 (Unchanged, Score: 0.6652)\n  - base_model.model.model.layers.6.mlp.down_proj: r=10 (Unchanged, Score: 0.6653)\n  - base_model.model.model.layers.7.self_attn.q_proj: r=12 (Unchanged, Score: 0.8025)\n  - base_model.model.model.layers.7.self_attn.k_proj: r=10 (Unchanged, Score: 0.5854)\n  - base_model.model.model.layers.7.self_attn.v_proj: r=9 (Unchanged, Score: 0.4964)\n  - base_model.model.model.layers.7.self_attn.o_proj: r=9 â†’ 8 (Score: 0.4344)\n  - base_model.model.model.layers.7.mlp.gate_proj: r=16 â†’ 14 (Score: 0.9486)\n  - base_model.model.model.layers.7.mlp.up_proj: r=12 â†’ 11 (Score: 0.6814)\n  - base_model.model.model.layers.7.mlp.down_proj: r=12 â†’ 11 (Score: 0.6814)\n  - base_model.model.model.layers.8.self_attn.q_proj: r=12 â†’ 11 (Score: 0.7037)\n  - base_model.model.model.layers.8.self_attn.k_proj: r=11 â†’ 9 (Score: 0.5602)\n  - base_model.model.model.layers.8.self_attn.v_proj: r=10 â†’ 9 (Score: 0.4807)\n  - base_model.model.model.layers.8.self_attn.o_proj: r=8 â†’ 7 (Score: 0.3472)\n  - base_model.model.model.layers.8.mlp.gate_proj: r=13 â†’ 12 (Score: 0.7749)\n  - base_model.model.model.layers.8.mlp.up_proj: r=11 â†’ 10 (Score: 0.6526)\n  - base_model.model.model.layers.8.mlp.down_proj: r=11 â†’ 10 (Score: 0.6528)\n  - base_model.model.model.layers.9.self_attn.q_proj: r=9 (Unchanged, Score: 0.5078)\n  - base_model.model.model.layers.9.self_attn.k_proj: r=11 â†’ 10 (Score: 0.6189)\n  - base_model.model.model.layers.9.self_attn.v_proj: r=11 â†’ 10 (Score: 0.5984)\n  - base_model.model.model.layers.9.self_attn.o_proj: r=8 (Unchanged, Score: 0.3647)\n  - base_model.model.model.layers.9.mlp.gate_proj: r=11 â†’ 10 (Score: 0.6510)\n  - base_model.model.model.layers.9.mlp.up_proj: r=10 (Unchanged, Score: 0.6284)\n  - base_model.model.model.layers.9.mlp.down_proj: r=10 (Unchanged, Score: 0.6286)\n  - base_model.model.model.layers.10.self_attn.q_proj: r=9 (Unchanged, Score: 0.5124)\n  - base_model.model.model.layers.10.self_attn.k_proj: r=11 â†’ 9 (Score: 0.5657)\n  - base_model.model.model.layers.10.self_attn.v_proj: r=9 â†’ 8 (Score: 0.4529)\n  - base_model.model.model.layers.10.self_attn.o_proj: r=9 â†’ 8 (Score: 0.4075)\n  - base_model.model.model.layers.10.mlp.gate_proj: r=11 (Unchanged, Score: 0.6774)\n  - base_model.model.model.layers.10.mlp.up_proj: r=10 â†’ 9 (Score: 0.5581)\n  - base_model.model.model.layers.10.mlp.down_proj: r=10 â†’ 9 (Score: 0.5580)\n  - base_model.model.model.layers.11.self_attn.q_proj: r=8 (Unchanged, Score: 0.3954)\n  - base_model.model.model.layers.11.self_attn.k_proj: r=16 â†’ 13 (Score: 0.8658)\n  - base_model.model.model.layers.11.self_attn.v_proj: r=10 â†’ 9 (Score: 0.5226)\n  - base_model.model.model.layers.11.self_attn.o_proj: r=9 (Unchanged, Score: 0.5170)\n  - base_model.model.model.layers.11.mlp.gate_proj: r=8 (Unchanged, Score: 0.4460)\n  - base_model.model.model.layers.11.mlp.up_proj: r=8 (Unchanged, Score: 0.4680)\n  - base_model.model.model.layers.11.mlp.down_proj: r=8 (Unchanged, Score: 0.4680)\n  - base_model.model.model.layers.12.self_attn.q_proj: r=7 (Unchanged, Score: 0.2423)\n  - base_model.model.model.layers.12.self_attn.k_proj: r=10 (Unchanged, Score: 0.6219)\n  - base_model.model.model.layers.12.self_attn.v_proj: r=9 â†’ 8 (Score: 0.4574)\n  - base_model.model.model.layers.12.self_attn.o_proj: r=6 (Unchanged, Score: 0.2004)\n  - base_model.model.model.layers.12.mlp.gate_proj: r=7 (Unchanged, Score: 0.3115)\n  - base_model.model.model.layers.12.mlp.up_proj: r=7 (Unchanged, Score: 0.3348)\n  - base_model.model.model.layers.12.mlp.down_proj: r=7 (Unchanged, Score: 0.3348)\n  - base_model.model.model.layers.13.self_attn.q_proj: r=6 (Unchanged, Score: 0.1833)\n  - base_model.model.model.layers.13.self_attn.k_proj: r=8 â†’ 9 (Score: 0.4834)\n  - base_model.model.model.layers.13.self_attn.v_proj: r=7 (Unchanged, Score: 0.3004)\n  - base_model.model.model.layers.13.self_attn.o_proj: r=6 (Unchanged, Score: 0.1320)\n  - base_model.model.model.layers.13.mlp.gate_proj: r=7 (Unchanged, Score: 0.2895)\n  - base_model.model.model.layers.13.mlp.up_proj: r=7 (Unchanged, Score: 0.2936)\n  - base_model.model.model.layers.13.mlp.down_proj: r=7 (Unchanged, Score: 0.2935)\n  - base_model.model.model.layers.14.self_attn.q_proj: r=6 (Unchanged, Score: 0.1467)\n  - base_model.model.model.layers.14.self_attn.k_proj: r=7 (Unchanged, Score: 0.2976)\n  - base_model.model.model.layers.14.self_attn.v_proj: r=6 â†’ 7 (Score: 0.2441)\n  - base_model.model.model.layers.14.self_attn.o_proj: r=6 (Unchanged, Score: 0.1392)\n  - base_model.model.model.layers.14.mlp.gate_proj: r=7 (Unchanged, Score: 0.2348)\n  - base_model.model.model.layers.14.mlp.up_proj: r=6 (Unchanged, Score: 0.2132)\n  - base_model.model.model.layers.14.mlp.down_proj: r=6 (Unchanged, Score: 0.2133)\n  - base_model.model.model.layers.15.self_attn.q_proj: r=6 (Unchanged, Score: 0.0927)\n  - base_model.model.model.layers.15.self_attn.k_proj: r=6 (Unchanged, Score: 0.1907)\n  - base_model.model.model.layers.15.self_attn.v_proj: r=6 (Unchanged, Score: 0.1396)\n  - base_model.model.model.layers.15.self_attn.o_proj: r=6 (Unchanged, Score: 0.0839)\n  - base_model.model.model.layers.15.mlp.gate_proj: r=6 (Unchanged, Score: 0.2185)\n  - base_model.model.model.layers.15.mlp.up_proj: r=6 (Unchanged, Score: 0.1742)\n  - base_model.model.model.layers.15.mlp.down_proj: r=6 (Unchanged, Score: 0.1742)\n  - base_model.model.model.layers.16.self_attn.q_proj: r=6 (Unchanged, Score: 0.1037)\n  - base_model.model.model.layers.16.self_attn.k_proj: r=6 (Unchanged, Score: 0.2328)\n  - base_model.model.model.layers.16.self_attn.v_proj: r=6 (Unchanged, Score: 0.1861)\n  - base_model.model.model.layers.16.self_attn.o_proj: r=6 (Unchanged, Score: 0.1063)\n  - base_model.model.model.layers.16.mlp.gate_proj: r=6 (Unchanged, Score: 0.1563)\n  - base_model.model.model.layers.16.mlp.up_proj: r=6 (Unchanged, Score: 0.1163)\n  - base_model.model.model.layers.16.mlp.down_proj: r=6 (Unchanged, Score: 0.1163)\n  - base_model.model.model.layers.17.self_attn.q_proj: r=6 (Unchanged, Score: 0.0943)\n  - base_model.model.model.layers.17.self_attn.k_proj: r=6 (Unchanged, Score: 0.1398)\n  - base_model.model.model.layers.17.self_attn.v_proj: r=6 (Unchanged, Score: 0.1031)\n  - base_model.model.model.layers.17.self_attn.o_proj: r=5 (Unchanged, Score: 0.0580)\n  - base_model.model.model.layers.17.mlp.gate_proj: r=6 (Unchanged, Score: 0.1628)\n  - base_model.model.model.layers.17.mlp.up_proj: r=6 (Unchanged, Score: 0.1380)\n  - base_model.model.model.layers.17.mlp.down_proj: r=6 (Unchanged, Score: 0.1380)\n  - base_model.model.model.layers.18.self_attn.q_proj: r=5 (Unchanged, Score: 0.0346)\n  - base_model.model.model.layers.18.self_attn.k_proj: r=6 (Unchanged, Score: 0.1732)\n  - base_model.model.model.layers.18.self_attn.v_proj: r=6 (Unchanged, Score: 0.1332)\n  - base_model.model.model.layers.18.self_attn.o_proj: r=5 (Unchanged, Score: 0.0274)\n  - base_model.model.model.layers.18.mlp.gate_proj: r=6 (Unchanged, Score: 0.1442)\n  - base_model.model.model.layers.18.mlp.up_proj: r=6 (Unchanged, Score: 0.1362)\n  - base_model.model.model.layers.18.mlp.down_proj: r=6 (Unchanged, Score: 0.1361)\n  - base_model.model.model.layers.19.self_attn.q_proj: r=5 (Unchanged, Score: 0.0357)\n  - base_model.model.model.layers.19.self_attn.k_proj: r=6 (Unchanged, Score: 0.1175)\n  - base_model.model.model.layers.19.self_attn.v_proj: r=6 (Unchanged, Score: 0.1125)\n  - base_model.model.model.layers.19.self_attn.o_proj: r=5 (Unchanged, Score: 0.0635)\n  - base_model.model.model.layers.19.mlp.gate_proj: r=6 (Unchanged, Score: 0.1331)\n  - base_model.model.model.layers.19.mlp.up_proj: r=6 (Unchanged, Score: 0.0925)\n  - base_model.model.model.layers.19.mlp.down_proj: r=6 (Unchanged, Score: 0.0925)\n  - base_model.model.model.layers.20.self_attn.q_proj: r=6 (Unchanged, Score: 0.0907)\n  - base_model.model.model.layers.20.self_attn.k_proj: r=6 (Unchanged, Score: 0.1505)\n  - base_model.model.model.layers.20.self_attn.v_proj: r=5 (Unchanged, Score: 0.0631)\n  - base_model.model.model.layers.20.self_attn.o_proj: r=5 (Unchanged, Score: 0.0208)\n  - base_model.model.model.layers.20.mlp.gate_proj: r=7 (Unchanged, Score: 0.2668)\n  - base_model.model.model.layers.20.mlp.up_proj: r=6 (Unchanged, Score: 0.1410)\n  - base_model.model.model.layers.20.mlp.down_proj: r=6 (Unchanged, Score: 0.1410)\n  - base_model.model.model.layers.21.self_attn.q_proj: r=5 (Unchanged, Score: 0.0000)\n  - base_model.model.model.layers.21.self_attn.k_proj: r=6 (Unchanged, Score: 0.1184)\n  - base_model.model.model.layers.21.self_attn.v_proj: r=6 (Unchanged, Score: 0.0849)\n  - base_model.model.model.layers.21.self_attn.o_proj: r=5 (Unchanged, Score: 0.0036)\n  - base_model.model.model.layers.21.mlp.gate_proj: r=6 (Unchanged, Score: 0.1018)\n  - base_model.model.model.layers.21.mlp.up_proj: r=6 (Unchanged, Score: 0.1125)\n  - base_model.model.model.layers.21.mlp.down_proj: r=6 (Unchanged, Score: 0.1125)\nâœ… AdaptiveLoRA: Rank setup for Epoch 3 complete.\n\nðŸ“„ Epoch 3: Rank allocations logged to ./logs/adaptive_lora_epoch_logs.csv\n\nSaving model and adapters...\nSaved model to ./tinyllama-qnli-lora\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"results=trainer.evaluate()\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T18:34:40.397711Z","iopub.execute_input":"2025-12-05T18:34:40.398038Z","iopub.status.idle":"2025-12-05T18:37:32.314302Z","shell.execute_reply.started":"2025-12-05T18:34:40.398010Z","shell.execute_reply":"2025-12-05T18:37:32.313696Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [47/47 02:48]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.4080371856689453, 'eval_accuracy': 0.82, 'eval_runtime': 171.907, 'eval_samples_per_second': 8.726, 'eval_steps_per_second': 0.273, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":18}]}